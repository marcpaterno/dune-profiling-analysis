
`%nin%` <- function(x, y) {
  !x %in% y
}

#' Read benchmark output generated by nanobench,
#'
#' This function expects the output in the style written for Linux machines,
#' which includes columns not collected on macOS.
#'
#' @param filename
#'
#' @return a tibble
#' @export
#'
read_benchmark <- function(filename)
{
  d <- read.so::read_md(filename)
  # Keep only the interesting columns
  d <- d[,c(1,3,4,7,10)]
  names(d) <- c("t","err","ins","branches", "name")
  d$err <- as.numeric(sub("%","",d$err))/100
  mutate(d,
         ins = as.integer(ins),
         branches = as.integer(branches),
         name = gsub("`","",name, fixed=TRUE),
         name = factor(name))
}

#' Read VTune data from my summary file
#'
#' In the resulting tibble, the column 'type' indicates what kind of measurement
#' is carried in the 't' column for that row.
#'     full: t is the time in seconds taken by this function in the full program
#'
#' @param filename
#'
#' @return a tibble
#' @export
#'
read_vtune <- function(filename)
{
  d <- read_csv(filename, col_types="ifdf")
  pivot_wider(d, names_from=func, values_from=t)
}

#' Read VTune hotspot reports
#'
#' @param filename
#'
#' @return a hotspot tibble
#' @export
#'
#' @examples
read_vtune_hotspots <- function(filename)
{
  checkmate::assert_file(filename)
  readr::read_tsv(filename, show_col_types=FALSE) |>
    rename(func = Function,
           t = `CPU Time`,
           teff = `CPU Time:Effective Time`,
           tspin = `CPU Time:Spin Time`,
           tspinimb = `CPU Time:Spin Time:Imbalance or Serial Spinning`,
           tspinlock = `CPU Time:Spin Time:Lock Contention`,
           tspinother = `CPU Time:Spin Time:Other`,
           tover = `CPU Time:Overhead Time`,
           tovercreate = `CPU Time:Overhead Time:Creation`,
           toversched = `CPU Time:Overhead Time:Scheduling`,
           toverred = `CPU Time:Overhead Time:Reduction`,
           toveratom = `CPU Time:Overhead Time:Atomics`,
           toverother = `CPU Time:Overhead Time:Other`,
           lib = Module,
           funcname = `Function (Full)`,
           file = `Source File`,
           addr = `Start Address`)
}

read_channel_data <- function(filename)
{
  x <- readr::read_tsv(filename, col_types = "iiiii")
  x
}

read_measurement_data <- function(filename)
{
  x <- readr::read_tsv(filename, col_types = "iiiiii")
  x
}

make_analysis_dataframes <- function(orig_channels, orig_meas)
{
  # Generate unique event ids "eid".
  # For some datasets this is trivial (we could have used just `event`), but that
  # is not true for all data sets. This is more robust.
  event_ids <-
    orig_channels |>
    select(run, subrun, event) |>
    unique() |>
    mutate(eid = row_number())

  # Add the eid into the per-channel data frame, and remove the now redundant
  # columns run, subrun, event. Do the same for the per-measurement data frame.
  channels <-
    left_join(orig_channels, event_ids) |>
    select(eid, channel, nmeas) |>
    mutate(eid = factor(eid), channel=factor(channel), nmeas = nmeas)
  meas <-
    left_join(orig_meas, event_ids) |>
    select(eid, channel, time, nphot) |>
    mutate(eid = factor(eid), channel=factor(channel), time = time, nphot = nphot)

  # Get the number of photons summed across all measurements in for each channel in each event.
  tmp <-
    group_by(meas, eid, channel) |>
    summarize(nphots = sum(nphot), .groups="drop")

  channels <- inner_join(channels, tmp)
  list(channels = channels, meas = meas)
}

#' Augment a dataframe with columns to allow plotting of conditioned box plots.
#'
#' @param df a dataframe
#' @param var a tidy-select column specification
#' @param nbins the number of bins to generate
#'
#' @return a tibble with two new columns, for the var's bin number and the var's
#'          median
#' @export
#'
augment_bins <- function(df, var, nbins=25)
{
  # Capture the argument as a quosure
  v <- enquo(var)
  # Get the string representation of the variable
  vname <- rlang::as_string(rlang::quo_get_expr(v))
  # add a new column named bin_column_name carrying the bin number for each row.
  tmp <- mutate(df, ibin = cut_number(!!v, nbins, labels=FALSE))
  # Determine the median of the variable var in each bin.
  tmp2 <- group_by(tmp, ibin) |>
    summarize(xxx = median(!!v), .groups="drop")
  # Give the new column a more appropriate name.
  names(tmp2)[length(tmp2)] <- paste0(vname, "_median")
  # Now augment the original dataframe with the new column
  tmp <- left_join(tmp, tmp2, by="ibin")
  names(tmp)[length(tmp)-1] <- paste0(vname, "_bin")
  tmp
}

read_dataframes <- function()
{
  run_names <- c("acosd", "acos4", "acos5", "orig")
  channels_files <- list.files(path="physics_validation_data", pattern=glob2rx("channels*.tsv"), full.names=TRUE)
  measurements_files <- list.files(path="physics_validation_data", pattern=glob2rx("measurements*.tsv.xz"), full.names = TRUE)
  channels <- lapply(channels_files, read_channel_data)
  measurements <- lapply(measurements_files, read_measurement_data)
  names(channels) <- run_names
  names(measurements) <- run_names
  channels <- bind_rows(channels, .id = "alg")
  channels$alg <- factor(channels$alg, levels=c("acosd","orig", "acos4", "acos5"), ordered=TRUE)
  measurements <- bind_rows(measurements, .id = "alg")
  chs <- pivot_wider(channels, id_cols=c(run, subrun, event,channel),names_from=alg, values_from=nmeas)
  chs <- mutate(chs,
                d4 = acos4-acosd,
                d5 = acos5-acosd,
                do = orig-acosd)
  list(channels=chs, measurements=measurements)
}